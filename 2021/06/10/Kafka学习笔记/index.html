<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Kafka学习笔记"><meta name="keywords" content="kafka"><meta name="author" content="Kaluna"><meta name="copyright" content="Kaluna"><title>Kafka学习笔记 | 嘿，不停转的轮子</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="嘿，不停转的轮子" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#分布式流处理平台Kafka大纲速览"><span class="toc-number">1.</span> <span class="toc-text">分布式流处理平台Kafka大纲速览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#技术技术栈和环境说明"><span class="toc-number">1.1.</span> <span class="toc-text">技术技术栈和环境说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#大纲速览"><span class="toc-number">1.2.</span> <span class="toc-text">大纲速览</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MQ消息中间件-JMS-AMQP核心知识"><span class="toc-number">2.</span> <span class="toc-text">MQ消息中间件+JMS+AMQP核心知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是MQ消息中间件和应用场景"><span class="toc-number">2.1.</span> <span class="toc-text">什么是MQ消息中间件和应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#JMS消息服务和和常见核心概念"><span class="toc-number">2.2.</span> <span class="toc-text">JMS消息服务和和常见核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AMQP高级消息队列协议和MQTT科普"><span class="toc-number">2.3.</span> <span class="toc-text">AMQP高级消息队列协议和MQTT科普</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#业界主流消息队列和技术选型"><span class="toc-number">2.4.</span> <span class="toc-text">业界主流消息队列和技术选型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka核心概念-安装部署实战"><span class="toc-number">3.</span> <span class="toc-text">Kafka核心概念+安装部署实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#分布式流处理平台kafka快速认知"><span class="toc-number">3.1.</span> <span class="toc-text">分布式流处理平台kafka快速认知</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分布式流处理平台Kafka核心概念"><span class="toc-number">3.2.</span> <span class="toc-text">分布式流处理平台Kafka核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka相关环境准备和安装JDK8"><span class="toc-number">3.3.</span> <span class="toc-text">Kafka相关环境准备和安装JDK8</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linux环境下Zookeeper和Kafka安装启动"><span class="toc-number">3.4.</span> <span class="toc-text">Linux环境下Zookeeper和Kafka安装启动</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka点对点-发布订阅模型讲解和写入存储流程实战"><span class="toc-number">4.</span> <span class="toc-text">Kafka点对点-发布订阅模型讲解和写入存储流程实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka命令行生产者发送消息和消费者消费消息实战"><span class="toc-number">4.1.</span> <span class="toc-text">Kafka命令行生产者发送消息和消费者消费消息实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka点对点模型和发布订阅模型"><span class="toc-number">4.2.</span> <span class="toc-text">Kafka点对点模型和发布订阅模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka消费者组配置实现点对点消费模型"><span class="toc-number">4.3.</span> <span class="toc-text">Kafka消费者组配置实现点对点消费模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka消费者组配置实现发布订阅消费模型"><span class="toc-number">4.4.</span> <span class="toc-text">Kafka消费者组配置实现发布订阅消费模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka数据存储流程和原理概述和LEO-HW"><span class="toc-number">4.5.</span> <span class="toc-text">Kafka数据存储流程和原理概述和LEO+HW</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SpringBoot2-X项目整合-Kafka核心API-Admin实战"><span class="toc-number">5.</span> <span class="toc-text">SpringBoot2.X项目整合-Kafka核心API-Admin实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SpringBoot2-X项目搭建整合Kafka客户端依赖配置"><span class="toc-number">5.1.</span> <span class="toc-text">SpringBoot2.X项目搭建整合Kafka客户端依赖配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SpringBoot2-x整合Kafka客户端-adminApi单元测试"><span class="toc-number">5.2.</span> <span class="toc-text">SpringBoot2.x整合Kafka客户端+adminApi单元测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka使用JavaAPI-AdminClient删除和列举topic"><span class="toc-number">5.3.</span> <span class="toc-text">Kafka使用JavaAPI-AdminClient删除和列举topic</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdminClientApi查看Topic详情和增加分区数量"><span class="toc-number">5.4.</span> <span class="toc-text">AdminClientApi查看Topic详情和增加分区数量</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka核心API生产者实战"><span class="toc-number">6.</span> <span class="toc-text">Kafka核心API生产者实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生产者发送到Broker分区策略和常见配置"><span class="toc-number">6.1.</span> <span class="toc-text">生产者发送到Broker分区策略和常见配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka核心API模块-producer-API实战"><span class="toc-number">6.2.</span> <span class="toc-text">Kafka核心API模块-producer API实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#【面试】-ProducerRecord介绍和key的作用"><span class="toc-number">6.3.</span> <span class="toc-text">【面试】 ProducerRecord介绍和key的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka核心API模块-producerAPI回调函数实战"><span class="toc-number">6.4.</span> <span class="toc-text">Kafka核心API模块-producerAPI回调函数实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#producer生产者发送指定分区实战"><span class="toc-number">6.5.</span> <span class="toc-text">producer生产者发送指定分区实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka生产者自定义partition分区规则实战"><span class="toc-number">6.6.</span> <span class="toc-text">Kafka生产者自定义partition分区规则实战</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka核心API消费者模块实战"><span class="toc-number">7.</span> <span class="toc-text">Kafka核心API消费者模块实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#【面试】Consumer消费者机制和分区策略"><span class="toc-number">7.1.</span> <span class="toc-text">【面试】Consumer消费者机制和分区策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#【面试】Consumer重新分配策略和offset维护机制"><span class="toc-number">7.2.</span> <span class="toc-text">【面试】Consumer重新分配策略和offset维护机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer配置和Kafka调试日志配置"><span class="toc-number">7.3.</span> <span class="toc-text">Consumer配置和Kafka调试日志配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka消费者Consumer消费消息配置实战"><span class="toc-number">7.4.</span> <span class="toc-text">Kafka消费者Consumer消费消息配置实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Consumer从头消费配置和手工提交offset配置"><span class="toc-number">7.5.</span> <span class="toc-text">Consumer从头消费配置和手工提交offset配置</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka数据文件存储-可靠性保证-ISR核心知识"><span class="toc-number">8.</span> <span class="toc-text">kafka数据文件存储-可靠性保证-ISR核心知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka数据存储流程和log日志"><span class="toc-number">8.1.</span> <span class="toc-text">Kafka数据存储流程和log日志</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#【核心】分布式系统的CAP理论"><span class="toc-number">8.2.</span> <span class="toc-text">【核心】分布式系统的CAP理论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka数据可靠性保证原理之副本Replica-ACK"><span class="toc-number">8.3.</span> <span class="toc-text">Kafka数据可靠性保证原理之副本Replica+ACK</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka的in-sync-replica-set机制"><span class="toc-number">8.4.</span> <span class="toc-text">Kafka的in-sync replica set机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka的HighWatermark的作用"><span class="toc-number">8.5.</span> <span class="toc-text">Kafka的HighWatermark的作用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#kafka高可用集群和高性能"><span class="toc-number">9.</span> <span class="toc-text">kafka高可用集群和高性能</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka高可用集群搭建节点需求规划"><span class="toc-number">9.1.</span> <span class="toc-text">Kafka高可用集群搭建节点需求规划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-ZooKeeper"><span class="toc-number">9.2.</span> <span class="toc-text">Kafka + ZooKeeper</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka高可用集群之zookeeper集群环境准备"><span class="toc-number">9.3.</span> <span class="toc-text">Kafka高可用集群之zookeeper集群环境准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka高可用集群搭建-环境准备"><span class="toc-number">9.4.</span> <span class="toc-text">Kafka高可用集群搭建-环境准备</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka高可用集群搭建实战-SpringBoot项目测试"><span class="toc-number">10.</span> <span class="toc-text">Kafka高可用集群搭建实战+SpringBoot项目测试</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka的中的日志数据清理"><span class="toc-number">10.1.</span> <span class="toc-text">Kafka的中的日志数据清理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka的高性能原理分析-ZeroCopy"><span class="toc-number">10.2.</span> <span class="toc-text">Kafka的高性能原理分析-ZeroCopy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka的高性能原理分析归纳总结"><span class="toc-number">10.3.</span> <span class="toc-text">Kafka的高性能原理分析归纳总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SpringBoot项目整合Spring-kafka和事务消息实战"><span class="toc-number">11.</span> <span class="toc-text">SpringBoot项目整合Spring-kafka和事务消息实战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Springboot项目整合spring-kafka依赖包配置"><span class="toc-number">11.1.</span> <span class="toc-text">Springboot项目整合spring-kafka依赖包配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Springboot项目整合spring-kafka监听消费消息"><span class="toc-number">11.2.</span> <span class="toc-text">Springboot项目整合spring-kafka监听消费消息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka事务消息-整合SpringBoot实战"><span class="toc-number">11.3.</span> <span class="toc-text">Kafka事务消息-整合SpringBoot实战</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#关于-Kafka的其他特性和技术选型建议"><span class="toc-number">11.4.</span> <span class="toc-text">关于 Kafka的其他特性和技术选型建议</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Kaluna</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/lxy1866" target="_blank" rel="noopener">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">20</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">28</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">11</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210521080208.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">嘿，不停转的轮子</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/about">About</a><a class="site-page" href="/tags/">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Kafka学习笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-06-10</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/kafka/">kafka</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2021/06/10/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2021/06/10/Kafka学习笔记/"></span></a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="分布式流处理平台Kafka大纲速览"><a href="#分布式流处理平台Kafka大纲速览" class="headerlink" title="分布式流处理平台Kafka大纲速览"></a>分布式流处理平台Kafka大纲速览</h1><h2 id="技术技术栈和环境说明"><a href="#技术技术栈和环境说明" class="headerlink" title="技术技术栈和环境说明"></a>技术技术栈和环境说明</h2><ul>
<li>Kafka版本：2.8.0</li>
<li>Scala版本：2.13</li>
<li>Zookeeper版本：3.7</li>
<li>SpringBoot.2.5 + Maven + IDEA旗舰版 + JDK8 或 JDK11</li>
</ul>
<h2 id="大纲速览"><a href="#大纲速览" class="headerlink" title="大纲速览"></a>大纲速览</h2><ul>
<li><p>高性能分布式流处理平台 Kafka核心知识+项目实战</p>
</li>
<li><p>Kakfa多种工作模式,点对点/发布订阅模型和应用场景</p>
</li>
<li><p>核心概念 生产者、消费者 Broker/Topic/Partition/leader/follower等</p>
</li>
<li><p>Linux服务器急速部署Zookeeper、Kafka，多种控制台操作指令，分区控制等</p>
</li>
<li><p>SpringBoot整合Kafka原生多个模块Admin/Producer/Consumer核心API+SpringKafka实战</p>
</li>
<li><p>高级篇-Kafka存储流程和原理讲解LEO+HW+Offset</p>
</li>
<li><p>高级篇-生产者发送消息模型、分区策略和核心配置实战，自定义分区Key策略等</p>
</li>
<li><p>高级篇-消费者消费消息模型、分区策略和重Rebalance实战等</p>
<p>高级篇-Broker数据文件存储模型-ACK和副本可靠性原理分析+ISR模型</p>
</li>
<li><p>高级篇-高可用搭建Zookeeper集群+Kafka集群+SpringBoot项目整合和故障演练</p>
</li>
<li><p>高级篇-Kafka高性能原理分析ZeroCopy+多案例事务消息实战+大数据技术栈路线</p>
</li>
<li><p>Kafka架构+设计思想+底层原理+互联网大厂面试题等</p>
</li>
</ul>
<h1 id="MQ消息中间件-JMS-AMQP核心知识"><a href="#MQ消息中间件-JMS-AMQP核心知识" class="headerlink" title="MQ消息中间件+JMS+AMQP核心知识"></a>MQ消息中间件+JMS+AMQP核心知识</h1><h2 id="什么是MQ消息中间件和应用场景"><a href="#什么是MQ消息中间件和应用场景" class="headerlink" title="什么是MQ消息中间件和应用场景"></a>什么是MQ消息中间件和应用场景</h2><ul>
<li>什么是MQ消息中间件<ul>
<li>全称MessageQueue，主要是用于程序和程序直接通信，异步+解耦</li>
</ul>
</li>
<li>使用场景：<ul>
<li>核心应用<ul>
<li>解耦：订单系统-》物流系统</li>
<li>异步：用户注册-》发送邮件，初始化信息</li>
<li>削峰：秒杀、日志处理</li>
</ul>
</li>
<li>跨平台 、多语言</li>
<li>分布式事务、最终一致性</li>
<li>RPC调用上下游对接，数据源变动-&gt;通知下属</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610110809.png" alt="image-20210610110754781"></p>
<h2 id="JMS消息服务和和常见核心概念"><a href="#JMS消息服务和和常见核心概念" class="headerlink" title="JMS消息服务和和常见核心概念"></a>JMS消息服务和和常见核心概念</h2><ul>
<li><p>什么是JMS: Java消息服务（Java Message Service),Java平台中关于面向消息中间件的接口</p>
<ul>
<li><p>JMS是一种与厂商无关的 API，用来访问消息收发系统消息，它类似于JDBC(Java Database Connectivity)。这里，JDBC 是可以用来访问许多不同关系数据库的 API</p>
</li>
<li><p>是由Sun公司早期提出的消息标准，旨在为java应用提供统一的消息操作，包括create、send、receive</p>
</li>
<li><p>JMS是针对java的，那微软开发了NMS（.NET消息传递服务）</p>
</li>
<li><p>特性</p>
<ul>
<li>面向Java平台的标准消息传递API</li>
<li>在Java或JVM语言比如Scala、Groovy中具有互用性</li>
<li>无需担心底层协议</li>
<li>有queues和topics两种消息传递模型</li>
<li>支持事务、能够定义消息格式（消息头、属性和内容）</li>
</ul>
</li>
<li><p>常见概念</p>
<ul>
<li>JMS提供者：连接面向消息中间件的，JMS接口的一个实现，RocketMQ,ActiveMQ,Kafka等等</li>
<li>JMS生产者(Message Producer)：生产消息的服务</li>
<li>JMS消费者(Message Consumer)：消费消息的服务</li>
<li>JMS消息：数据对象</li>
<li>JMS队列：存储待消费消息的区域</li>
<li>JMS主题：一种支持发送消息给多个订阅者的机制</li>
<li>JMS消息通常有两种类型：点对点（Point-to-Point)、发布/订阅（Publish/Subscribe）</li>
</ul>
</li>
<li><p>基础编程模型</p>
<ul>
<li>MQ中需要用的一些类</li>
<li>ConnectionFactory ：连接工厂，JMS 用它创建连接</li>
<li>Connection ：JMS 客户端到JMS Provider 的连接</li>
<li>Session： 一个发送或接收消息的线程</li>
<li>Destination ：消息的目的地;消息发送给谁.</li>
<li>MessageConsumer / MessageProducer： 消息消费者，消息生产者</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610110915.png" alt="image-20210610110913772"></p>
<h2 id="AMQP高级消息队列协议和MQTT科普"><a href="#AMQP高级消息队列协议和MQTT科普" class="headerlink" title="AMQP高级消息队列协议和MQTT科普"></a>AMQP高级消息队列协议和MQTT科普</h2><ul>
<li>背景<ul>
<li>JMS或者NMS都没有标准的底层协议，API是与编程语言绑定的，每个消息队列厂商就存在多种不同格式规范的产品，对使用者就产生了很多问题, AMQP解决了这个问题，它使用了一套标准的底层协议</li>
</ul>
</li>
<li>什么是AMQP<ul>
<li>AMQP（advanced message queuing protocol）在2003年时被提出，最早用于解决金融领不同平台之间的消息传递交互问题,就是是一种协议，兼容JMS</li>
<li>更准确说的链接协议 binary- wire-level-protocol 直接定义网络交换的数据格式，类似http</li>
<li>具体的产品实现比较多，RabbitMQ就是其中一种</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610111447.png" alt="image-20210610111444397"></p>
<ul>
<li>特性<ul>
<li>独立于平台的底层消息传递协议</li>
<li>消费者驱动消息传递</li>
<li>跨语言和平台的互用性、属于底层协议</li>
<li>有5种交换类型direct，fanout，topic，headers，system</li>
<li>面向缓存的、可实现高性能、支持经典的消息队列，循环，存储和转发</li>
<li>支持长周期消息传递、支持事务（跨消息队列）</li>
</ul>
</li>
<li>AMQP和JMS的主要区别<ul>
<li>AMQP不从API层进行限定，直接定义网络交换的数据格式,这使得实现了AMQP的provider天然性就是跨平台</li>
<li>比如Java语言产生的消息，可以用其他语言比如python的进行消费</li>
<li>AQMP可以用http来进行类比，不关心实现接口的语言，只要都按照相应的数据格式去发送报文请求，不同语言的client可以和不同语言的server进行通讯</li>
<li>JMS消息类型：TextMessage/ObjectMessage/StreamMessage等</li>
<li>AMQP消息类型：Byte[]</li>
</ul>
</li>
<li>MQTT<ul>
<li>MQTT: 消息队列遥测传输（Message Queueing Telemetry Transport ）</li>
<li>背景：<ul>
<li>我们有面向基于Java的企业应用的JMS和面向所有其他应用需求的AMQP，那这个MQTT是做啥的？</li>
</ul>
</li>
<li>原因<ul>
<li>计算性能不高的设备不能适应AMQP上的复杂操作,MQTT它是专门为小设备设计的</li>
<li>MQTT主要是是物联网（IOT）中大量的使用</li>
</ul>
</li>
<li>特性<ul>
<li>内存占用低，为小型无声设备之间通过低带宽发送短消息而设计</li>
<li>不支持长周期存储和转发，不允许分段消息（很难发送长消息）</li>
<li>支持主题发布-订阅、不支持事务（仅基本确认）</li>
<li>消息实际上是短暂的（短周期）</li>
<li>简单用户名和密码、不支持安全连接、消息不透明</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="业界主流消息队列和技术选型"><a href="#业界主流消息队列和技术选型" class="headerlink" title="业界主流消息队列和技术选型"></a>业界主流消息队列和技术选型</h2><ul>
<li><p>业界主流的消息队列：Apache ActiveMQ、Kafka、RabbitMQ、RocketMQ</p>
<ul>
<li><p>ActiveMQ：<a href="http://activemq.apache.org/" target="_blank" rel="noopener">http://activemq.apache.org/</a></p>
<ul>
<li><p>Apache出品，历史悠久，支持多种语言的客户端和协议，支持多种语言Java, .NET, C++ 等</p>
</li>
<li><p>基于JMS Provider的实现</p>
</li>
<li><p>缺点：吞吐量不高，多队列的时候性能下降，存在消息丢失的情况，比较少大规模使用</p>
</li>
</ul>
</li>
<li><p>Kafka：<a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a></p>
<ul>
<li><p>是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统(严格意义上是不属于队列产品，是一个流处理平台)，它可以处理大规模的网站中的所有动作流数据(网页浏览，搜索和其他用户的行动)，副本集机制，实现数据冗余，保障数据尽量不丢失；支持多个生产者和消费者</p>
</li>
<li><p>类似MQ，功能较为简单，主要支持常规的MQ功能</p>
</li>
<li><p>它提供了类似于JMS的特性，但是在设计实现上完全不同，它并不是JMS规范的实现</p>
</li>
<li><p>缺点：运维难度大，文档比较少, 需要掌握Scala</p>
</li>
</ul>
</li>
<li><p>RocketMQ：<a href="http://rocketmq.apache.org/" target="_blank" rel="noopener">http://rocketmq.apache.org/</a></p>
<ul>
<li>阿里开源的一款的消息中间件, 纯Java开发，具有高吞吐量、高可用性、适合大规模分布式系统应用的特点, 性能强劲(零拷贝技术)，支持海量堆积, 支持指定次数和时间间隔的失败消息重发,支持consumer端tag过滤、延迟消息等，在阿里内部进行大规模使用，适合在电商，互联网金融等领域</li>
<li>基于JMS Provider的实现</li>
<li>缺点：社区相对不活跃，更新比较快，纯java支持</li>
</ul>
</li>
<li><p>RabbitMQ：<a href="http://www.rabbitmq.com/" target="_blank" rel="noopener">http://www.rabbitmq.com/</a></p>
<ul>
<li>是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、C、用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不错</li>
<li>缺点：使用Erlang开发，阅读和修改源码难度大</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Kafka核心概念-安装部署实战"><a href="#Kafka核心概念-安装部署实战" class="headerlink" title="Kafka核心概念+安装部署实战"></a>Kafka核心概念+安装部署实战</h1><h2 id="分布式流处理平台kafka快速认知"><a href="#分布式流处理平台kafka快速认知" class="headerlink" title="分布式流处理平台kafka快速认知"></a>分布式流处理平台kafka快速认知</h2><p>Kafka</p>
<ul>
<li><p>Kafka是最初由Linkedin公司开发，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目，也是一个开源【分布式流处理平台】，由Scala和Java编写，（也当做MQ系统，但不是纯粹的消息系统）</p>
<ul>
<li>open-source distributed event streaming platform</li>
</ul>
</li>
<li><p>核心：一种高吞吐量的分布式流处理平台，它可以处理消费者在网站中的所有动作流数据。</p>
<ul>
<li>比如 网页浏览，搜索和其他用户的行为等，应用于大数据实时处理领域</li>
</ul>
</li>
<li><p>官网：<a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a></p>
</li>
<li><p>快速开始：<a href="http://kafka.apache.org/quickstart" target="_blank" rel="noopener">http://kafka.apache.org/quickstart</a></p>
</li>
<li><p>快速认识概念</p>
<ul>
<li>Broker<ul>
<li>Kafka的服务端程序，可以认为一个mq节点就是一个broker</li>
<li>broker存储topic的数据</li>
</ul>
</li>
<li>Producer生产者<ul>
<li>创建消息Message，然后发布到MQ中</li>
<li>该角色将消息发布到Kafka的topic中</li>
</ul>
</li>
<li>Consumer消费者:<ul>
<li>消费队列里面的消息</li>
</ul>
</li>
</ul>
<p><img src="https://file.xdclass.net/note/2021/kafka/img/Untitled%20Diagram.png" alt="Untitled Diagram"></p>
</li>
</ul>
<h2 id="分布式流处理平台Kafka核心概念"><a href="#分布式流处理平台Kafka核心概念" class="headerlink" title="分布式流处理平台Kafka核心概念"></a>分布式流处理平台Kafka核心概念</h2><p>核心概念</p>
<ul>
<li>Broker<ul>
<li>Kafka的服务端程序，可以认为一个mq节点就是一个broker</li>
<li>broker存储topic的数据</li>
</ul>
</li>
<li>Producer生产者<ul>
<li>创建消息Message，然后发布到MQ中</li>
<li>该角色将消息发布到Kafka的topic中</li>
</ul>
</li>
<li>Consumer消费者:<ul>
<li>消费队列里面的消息</li>
</ul>
</li>
<li>ConsumerGroup消费者组<ul>
<li>同个topic, 广播发送给不同的group，一个group中只有一个consumer可以消费此消息</li>
</ul>
</li>
<li>Topic<ul>
<li>每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic，主题的意思</li>
</ul>
</li>
<li>Partition分区<ul>
<li>kafka数据存储的基本单元，topic中的数据分割为一个或多个partition，每个topic至少有一个partition，是有序的</li>
<li>一个Topic的多个partitions, 被分布在kafka集群中的多个server上</li>
<li>消费者数量 &lt;=小于或者等于Partition数量</li>
</ul>
</li>
<li>Replication 副本（备胎）<ul>
<li>同个Partition会有多个副本replication ，多个副本的数据是一样的，当其他broker挂掉后，系统可以主动用副本提供服务</li>
<li>默认每个topic的副本都是1（默认是没有副本，节省资源），也可以在创建topic的时候指定</li>
<li>如果当前kafka集群只有3个broker节点，则replication-factor最大就是3了，如果创建副本为4，则会报错</li>
</ul>
</li>
<li>ReplicationLeader、ReplicationFollower<ul>
<li>Partition有多个副本，但只有一个replicationLeader负责该Partition和生产者消费者交互</li>
<li>ReplicationFollower只是做一个备份，从replicationLeader进行同步</li>
</ul>
</li>
<li>ReplicationManager<ul>
<li>负责Broker所有分区副本信息，Replication 副本状态切换</li>
</ul>
</li>
<li>offset<ul>
<li>每个consumer实例需要为他消费的partition维护一个记录自己消费到哪里的偏移offset</li>
<li>kafka把offset保存在消费端的消费者组里</li>
</ul>
</li>
<li>特点总结<ul>
<li>多订阅者<ul>
<li>一个topic可以有一个或者多个订阅者</li>
<li>每个订阅者都要有一个partition，所以订阅者数量要少于等于partition数量</li>
</ul>
</li>
<li>高吞吐量、低延迟: 每秒可以处理几十万条消息</li>
<li>高并发：几千个客户端同时读写</li>
<li>容错性：多副本、多分区，允许集群中节点失败，如果副本数据量为n,则可以n-1个节点失败</li>
<li>扩展性强：支持热扩展</li>
</ul>
</li>
<li>基于消费者组可以实现：<ul>
<li>基于队列的模型：所有消费者都在同一消费者组里，每条消息只会被一个消费者处理</li>
<li>基于发布订阅模型：消费者属于不同的消费者组，假如每个消费者都有自己的消费者组，这样kafka消息就能广播到所有消费者实例上</li>
</ul>
</li>
</ul>
<h2 id="Kafka相关环境准备和安装JDK8"><a href="#Kafka相关环境准备和安装JDK8" class="headerlink" title="Kafka相关环境准备和安装JDK8"></a>Kafka相关环境准备和安装JDK8</h2><ul>
<li><p>需要的软件和环境版本说明</p>
<ul>
<li>kafka-xx-yy<ul>
<li>xx 是scala版本，yy是kafka版本（scala是基于jdk开发，需要安装jdk环境）</li>
<li>下载地址：<a href="http://kafka.apache.org/downloads" target="_blank" rel="noopener">http://kafka.apache.org/downloads</a></li>
</ul>
</li>
<li>zookeeper<ul>
<li>Apache 软件基金会的一个软件项目，它为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册</li>
<li>下载地址：<a href="https://zookeeper.apache.org/releases.html" target="_blank" rel="noopener">https://zookeeper.apache.org/releases.html</a></li>
</ul>
</li>
<li>jdk1.8</li>
</ul>
</li>
<li><p>步骤</p>
<ul>
<li><p>上传安装包（zk、jdk、kafka）</p>
</li>
<li><p>安装jdk</p>
<ul>
<li><p>配置全局环境变量</p>
<ul>
<li><p>解压：tar -zxvf jdk-8u181-linux-x64.tar.gz</p>
</li>
<li><p>重命名</p>
</li>
<li><p>vim /etc/profile</p>
</li>
<li><p>配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/local/software/jdk1.8</span><br><span class="line">CLASSPATH=$JAVA_HOME/lib/</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export PATH JAVA_HOME CLASSPATH</span><br></pre></td></tr></table></figure>
</li>
<li><p>环境变量立刻生效</p>
<ul>
<li>source /etc/profile</li>
</ul>
</li>
</ul>
</li>
<li><p>查看安装情况 java -version</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Linux环境下Zookeeper和Kafka安装启动"><a href="#Linux环境下Zookeeper和Kafka安装启动" class="headerlink" title="Linux环境下Zookeeper和Kafka安装启动"></a>Linux环境下Zookeeper和Kafka安装启动</h2><ul>
<li><p>安装Zookeeper (默认2181端口)</p>
<ul>
<li>默认配置文件 zoo.cfg</li>
<li>启动zk<ul>
<li>bin/zkServer.sh start</li>
</ul>
</li>
</ul>
</li>
<li><p>安装Kafka (默认 9092端口)</p>
<ul>
<li><p>config目录下 server.properties</p>
<img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610113205.png" alt="image-20210610113152510" style="zoom: 67%;" />

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标识broker编号，集群中有多个broker，则每个broker的编号需要设置不同</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#修改下面两个配置 ( listeners 配置的ip和advertised.listeners相同时启动kafka会报错)</span></span><br><span class="line"><span class="attr">listeners(内网Ip)</span></span><br><span class="line"><span class="attr">advertised.listeners(公网ip)</span></span><br><span class="line"><span class="comment">#修改zk地址,默认地址</span></span><br><span class="line"><span class="meta">zookeeper.connection</span>=<span class="string">localhost:2181</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>bin目录启动和停止</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动</span></span><br><span class="line">./kafka-server-start.sh  ../config/server.properties &amp;</span><br><span class="line"><span class="comment">#停止</span></span><br><span class="line">kafka-server-stop.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --create --zookeeper 101.132.252.118:2181 --replication-factor 1 --partitions 1 --topic xdclass-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --list --zookeeper 101.132.252.118:2181</span><br></pre></td></tr></table></figure>
</li>
<li><p>Linux环境下daemon守护进程运行Kafka</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-server-start.sh -daemon ../config/server.properties &amp;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h1 id="Kafka点对点-发布订阅模型讲解和写入存储流程实战"><a href="#Kafka点对点-发布订阅模型讲解和写入存储流程实战" class="headerlink" title="Kafka点对点-发布订阅模型讲解和写入存储流程实战"></a>Kafka点对点-发布订阅模型讲解和写入存储流程实战</h1><h2 id="Kafka命令行生产者发送消息和消费者消费消息实战"><a href="#Kafka命令行生产者发送消息和消费者消费消息实战" class="headerlink" title="Kafka命令行生产者发送消息和消费者消费消息实战"></a>Kafka命令行生产者发送消息和消费者消费消息实战</h2><ul>
<li><p>创建topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/software/kafka/bin</span><br><span class="line">./kafka-topics.sh --create --zookeeper 101.132.252.118:2181 --replication-factor 1 --partitions 2 --topic xdclass-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --list --zookeeper 101.132.252.118:2181</span><br></pre></td></tr></table></figure>
</li>
<li><p>生产者发送消息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-producer.sh --broker-list 101.132.252.118:9092  --topic version1-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者消费消息 ( –from-beginning：会把主题中以往所有的数据都读取出来, 重启后会有这个重复消费）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 101.132.252.118:9092 --from-beginning --topic version1-topic</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610114448.png" alt="image-20210610114445032"></p>
</li>
<li><p>删除topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper 101.132.252.118:2181 --delete --topic t1</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看broker节点topic状态信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --describe --zookeeper 101.132.252.118:2181  --topic xdclass-topic</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h2 id="Kafka点对点模型和发布订阅模型"><a href="#Kafka点对点模型和发布订阅模型" class="headerlink" title="Kafka点对点模型和发布订阅模型"></a>Kafka点对点模型和发布订阅模型</h2><ul>
<li>JMS规范目前支持两种消息模型<ul>
<li>点对点（point to point)<ul>
<li>消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息</li>
<li>消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。 Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费</li>
</ul>
</li>
<li>发布/订阅（publish/subscribe）<ul>
<li>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。</li>
<li>和点对点方式不同，发布到topic的消息会被所有订阅者消费。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610130420.png" alt="image-20210610130417875"></p>
<h2 id="Kafka消费者组配置实现点对点消费模型"><a href="#Kafka消费者组配置实现点对点消费模型" class="headerlink" title="Kafka消费者组配置实现点对点消费模型"></a>Kafka消费者组配置实现点对点消费模型</h2><ul>
<li><p>编辑消费者配置（确保同个名称group.id一样）</p>
<ul>
<li>编辑 config/consumer.properties</li>
</ul>
</li>
<li><p>创建topic, 1个分区</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --create --zookeeper 112.74.55.160:2181 --replication-factor 1 --partitions 2 --topic xdclass-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定配置文件启动 两个消费者</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 101.132.252.118:9092 --from-beginning --topic xdclass-topic --consumer.config ../config/consumer.properties</span><br></pre></td></tr></table></figure>
</li>
<li><p>现象</p>
<ul>
<li>只有一个消费者可以消费到数据，一个分区只能被同个消费者组下的某个消费者进行消费</li>
</ul>
</li>
</ul>
<img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210610132352.png" alt="image-20210610132337485" style="zoom: 67%;" />

<h2 id="Kafka消费者组配置实现发布订阅消费模型"><a href="#Kafka消费者组配置实现发布订阅消费模型" class="headerlink" title="Kafka消费者组配置实现发布订阅消费模型"></a>Kafka消费者组配置实现发布订阅消费模型</h2><ul>
<li><p>编辑消费者配置（确保group.id 不一样）</p>
<ul>
<li>cp config/consumer.properties config/consumer-1.properties</li>
<li>cp config/consumer.properties config/consumer-2.properties</li>
<li>编辑 config/consumer-1.properties</li>
<li>编辑 config/consumer-2.properties</li>
</ul>
</li>
<li><p>创建topic, 2个分区</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --create --zookeeper 101.132.252.118:2181 --replication-factor 1 --partitions 1--topic xdclass-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>生产者生产消息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-producer.sh --broker-list 101.132.252.118:9092  --topic xdclass-topic</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定配置文件启动两个消费者</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server 101.132.252.118:9092 --from-beginning --topic xdclass-topic --consumer.config ../config/consumer1.properties</span><br><span class="line"></span><br><span class="line">./kafka-console-consumer.sh --bootstrap-server 101.132.252.118:9092 --from-beginning --topic xdclass-topic --consumer.config ../config/consumer2.properties</span><br></pre></td></tr></table></figure>
</li>
<li><p>现象</p>
<ul>
<li>两个不同消费者组的节点，都可以消费到消息，实现发布订阅模型</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613152149.png" alt="image-20210610134550679"></p>
<h2 id="Kafka数据存储流程和原理概述和LEO-HW"><a href="#Kafka数据存储流程和原理概述和LEO-HW" class="headerlink" title="Kafka数据存储流程和原理概述和LEO+HW"></a>Kafka数据存储流程和原理概述和LEO+HW</h2><p><strong>Partition</strong></p>
<ul>
<li>topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列</li>
<li>是以文件夹的形式存储在具体Broker本机上</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613152455.png" alt="image-20210613152452852"></p>
<p><strong>LEO（LogEndOffset）</strong></p>
<ul>
<li>表示每个partition的log最后一条Message的位置。</li>
</ul>
<p><strong>HW（HighWatermark）</strong></p>
<ul>
<li>表示partition各个replicas数据间同步且一致的offset位置，即表示allreplicas已经commit的位置</li>
<li>HW之前的数据才是Commit后的，对消费者才可见</li>
<li>ISR集合里面最小leo</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613152603.png" alt="image-20210613152600594"></p>
<p><strong>offset</strong>：</p>
<ul>
<li>每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中</li>
<li>partition中的每个消息都有一个连续的序列号叫做offset，用于partition唯一标识一条消息</li>
<li>可以认为offset是partition中Message的id</li>
</ul>
<p><strong>Segment</strong>：每个partition又由多个segment file组成；</p>
<ul>
<li>segment file 由2部分组成，分别为index file和data file（log file），</li>
<li>两个文件是一一对应的，后缀”.index”和”.log”分别表示索引文件和数据文件</li>
<li>命名规则：partition的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset+1</li>
</ul>
<p><strong>Kafka高效文件存储设计特点：</strong></p>
<ul>
<li>Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</li>
<li>通过索引信息可以快速定位message</li>
<li>producer生产数据，要写入到log文件中，写的过程中一直追加到文件末尾，为顺序写，官网数据表明。同样的磁盘，顺序写能到600M/S，而随机写只有100K/S</li>
</ul>
<h1 id="SpringBoot2-X项目整合-Kafka核心API-Admin实战"><a href="#SpringBoot2-X项目整合-Kafka核心API-Admin实战" class="headerlink" title="SpringBoot2.X项目整合-Kafka核心API-Admin实战"></a>SpringBoot2.X项目整合-Kafka核心API-Admin实战</h1><h2 id="SpringBoot2-X项目搭建整合Kafka客户端依赖配置"><a href="#SpringBoot2-X项目搭建整合Kafka客户端依赖配置" class="headerlink" title="SpringBoot2.X项目搭建整合Kafka客户端依赖配置"></a><strong>SpringBoot2.X项目搭建整合Kafka客户端依赖配置</strong></h2><p>新版SpringBoot2.X介绍</p>
<ul>
<li>官网：<a href="https://spring.io/projects/spring-boot" target="_blank" rel="noopener">https://spring.io/projects/spring-boot</a></li>
<li>GitHub地址：<a href="https://github.com/spring-projects/spring-boot" target="_blank" rel="noopener">https://github.com/spring-projects/spring-boot</a></li>
<li>官方文档：<a href="https://spring.io/guides/gs/spring-boot/" target="_blank" rel="noopener">https://spring.io/guides/gs/spring-boot/</a></li>
<li>视频地址：<a href="https://item.taobao.com/item.htm?id=618384570391" target="_blank" rel="noopener">https://item.taobao.com/item.htm?id=618384570391</a></li>
</ul>
<p>相关软件环境和作用</p>
<ul>
<li>JDK1.8+以上</li>
<li>Maven3.5+</li>
<li>编辑器IDEA(旗舰版)</li>
</ul>
<p>在线创建 ：<a href="https://start.spring.io/" target="_blank" rel="noopener">https://start.spring.io/</a></p>
<ul>
<li>注意：<ul>
<li>采用springboot2.5 + jdk11</li>
<li>初次导入项目下载包比较慢 5~20分钟不等<ul>
<li>出问题的话: mvn clean install 试试</li>
</ul>
</li>
<li>不建议修改默认maven仓库（可以先还原默认的，防止下载包失败）</li>
<li>idea记得配置jdk11</li>
</ul>
</li>
</ul>
<p>在SpringBoot整合kafka很简单</p>
<ul>
<li><p>添加依赖 kafka-clients</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="SpringBoot2-x整合Kafka客户端-adminApi单元测试"><a href="#SpringBoot2-x整合Kafka客户端-adminApi单元测试" class="headerlink" title="SpringBoot2.x整合Kafka客户端+adminApi单元测试"></a>SpringBoot2.x整合Kafka客户端+adminApi单元测试</h2><p>单元测试配置客户端+创建topic</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 设置admin 客户端</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> AdminClient <span class="title">initAdminClient</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.setProperty(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"101.132.252.118:9092"</span>);</span><br><span class="line"></span><br><span class="line">    AdminClient adminClient = AdminClient.create(properties);</span><br><span class="line">    <span class="keyword">return</span> adminClient;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//创建</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">createTopic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient adminClient = initAdminClient();</span><br><span class="line">    <span class="comment">// 2个分区，1个副本</span></span><br><span class="line">    NewTopic newTopic = <span class="keyword">new</span> NewTopic(TOPIC_NAME, <span class="number">2</span> , (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    CreateTopicsResult createTopicsResult = adminClient.createTopics(Arrays.asList(newTopic));</span><br><span class="line">    <span class="comment">//future等待创建，成功不会有任何报错，如果创建失败和超时会报错。</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        createTopicsResult.all().get();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">"创建新的topic"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>查看topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --list --zookeeper 101.132.252.118:2181</span><br></pre></td></tr></table></figure>

<p>查看broker节点topic状态信息</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --describe --zookeeper 101.132.252.118:2181  --topic xdclass-sp-topic-test</span><br></pre></td></tr></table></figure>

<h2 id="Kafka使用JavaAPI-AdminClient删除和列举topic"><a href="#Kafka使用JavaAPI-AdminClient删除和列举topic" class="headerlink" title="Kafka使用JavaAPI-AdminClient删除和列举topic"></a>Kafka使用JavaAPI-AdminClient删除和列举topic</h2><p>list列举</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">listTopic</span><span class="params">()</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">    AdminClient adminClient = initAdminClient();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//是否查看内部的topic,可以不用</span></span><br><span class="line">    ListTopicsOptions options = <span class="keyword">new</span> ListTopicsOptions();</span><br><span class="line">    options.listInternal(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    ListTopicsResult listTopics = adminClient.listTopics(options);</span><br><span class="line">    Set&lt;String&gt; topics = listTopics.names().get();</span><br><span class="line">    <span class="keyword">for</span> (String topic : topics) &#123;</span><br><span class="line">        System.err.println(topic);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>删除</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//删除</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">delTopicTest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AdminClient adminClient = initAdminClient();</span><br><span class="line">    DeleteTopicsResult deleteTopicsResult = adminClient.deleteTopics(Arrays.asList(<span class="string">"xdclass-sp11-topic"</span>));</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        deleteTopicsResult.all().get();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="AdminClientApi查看Topic详情和增加分区数量"><a href="#AdminClientApi查看Topic详情和增加分区数量" class="headerlink" title="AdminClientApi查看Topic详情和增加分区数量"></a>AdminClientApi查看Topic详情和增加分区数量</h2><p>查看topic详情</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取指定topic的详细信息</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">getTopicInfo</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    AdminClient adminClient = initAdminClient();</span><br><span class="line">    DescribeTopicsResult describeTopicsResult = adminClient.describeTopics(Arrays.asList(TOPIC_NAME));</span><br><span class="line"></span><br><span class="line">    Map&lt;String, TopicDescription&gt; stringTopicDescriptionMap = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">    Set&lt;Map.Entry&lt;String, TopicDescription&gt;&gt; entries = stringTopicDescriptionMap.entrySet();</span><br><span class="line"></span><br><span class="line">    entries.stream().forEach((entry)-&gt; System.out.println(<span class="string">"name ："</span>+entry.getKey()+<span class="string">" , desc: "</span>+ entry.getValue()));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>增加分区数量</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 增加分区数量</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 如果当主题中的消息包含有key时(即key不为null)，根据key来计算分区的行为就会有所影响消息顺序性</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 注意：Kafka中的分区数只能增加不能减少，减少的话数据不知怎么处理</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span>  <span class="keyword">void</span> <span class="title">incrPartitionsTest</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">    Map&lt;String, NewPartitions&gt; infoMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    NewPartitions newPartitions = NewPartitions.increaseTo(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">    AdminClient adminClient = initAdminClient();</span><br><span class="line">    infoMap.put(TOPIC_NAME, newPartitions);</span><br><span class="line"></span><br><span class="line">    CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(infoMap);</span><br><span class="line">    createPartitionsResult.all().get();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Kafka核心API生产者实战"><a href="#Kafka核心API生产者实战" class="headerlink" title="Kafka核心API生产者实战"></a>Kafka核心API生产者实战</h1><h2 id="生产者发送到Broker分区策略和常见配置"><a href="#生产者发送到Broker分区策略和常见配置" class="headerlink" title="生产者发送到Broker分区策略和常见配置"></a>生产者发送到Broker分区策略和常见配置</h2><p>生产者发送到broker里面的流程是怎样的呢，一个 topic 有多个 partition分区，每个分区又有多个副本</p>
<ul>
<li>如果指定Partition ID,则PR被发送至指定Partition (ProducerRecord)</li>
<li>如果未指定Partition ID,但指定了Key, PR会按照hash(key)发送至对应Partition</li>
<li>如果未指定Partition ID也没指定Key，PR会按照默认 round-robin轮训模式发送到每个Partition<ul>
<li>消费者消费partition分区默认是range模式</li>
</ul>
</li>
<li>如果同时指定了Partition ID和Key, PR只会发送到指定的Partition (Key不起作用，代码逻辑决定)</li>
<li>注意：Partition有多个副本，但只有一个replicationLeader负责该Partition和生产者消费者交互</li>
</ul>
<p>生产者到broker发送流程</p>
<ul>
<li>Kafka的客户端发送数据到服务器，不是来一条就发一条，会经过内存缓冲区（默认是16KB），通过KafkaProducer发送出去的消息都是先进入到客户端本地的内存缓冲里，然后把很多消息收集到的Batch里面，再一次性发送到Broker上去的，这样性能才可能题高</li>
</ul>
<p>生产者常见配置</p>
<ul>
<li>官方文档 <a href="http://kafka.apache.org/documentation/#producerconfigs" target="_blank" rel="noopener">http://kafka.apache.org/documentation/#producerconfigs</a></li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#kafka地址,即broker地址</span></span><br><span class="line"><span class="meta">bootstrap.servers</span>  <span class="string"></span></span><br><span class="line"></span><br><span class="line"><span class="comment">#当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别,分别是0, 1，all。</span></span><br><span class="line"><span class="attr">acks</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#请求失败，生产者会自动重试，指定是0次，如果启用重试，则会有重复消息的可能性</span></span><br><span class="line"><span class="attr">retries</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#每个分区未发送消息总字节大小,单位：字节，超过设置的值就会提交数据到服务端，默认值是16KB</span></span><br><span class="line"><span class="attr">batch.size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认值就是0，消息是立刻发送的，即便batch.size缓冲空间还没有满，如果想减少请求的数量，可以设置 linger.ms 大于#0，即消息在缓冲区保留的时间，超过设置的值就会被提交到服务端</span></span><br><span class="line"><span class="comment"># 通俗解释是，本该早就发出去的消息被迫至少等待了linger.ms时间，相对于这时间内积累了更多消息，批量发送 减少请求</span></span><br><span class="line"><span class="comment">#如果batch被填满或者linger.ms达到上限，满足其中一个就会被发送</span></span><br><span class="line"><span class="attr">linger.ms</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># buffer.memory的用来约束Kafka Producer能够使用的内存缓冲的大小的，默认值32MB。</span></span><br><span class="line"><span class="comment"># 如果buffer.memory设置的太小，可能导致消息快速的写入内存缓冲里，但Sender线程来不及把消息发送到Kafka服务器</span></span><br><span class="line"><span class="comment"># 会造成内存缓冲很快就被写满，而一旦被写满，就会阻塞用户线程，不让继续往Kafka写消息了</span></span><br><span class="line"><span class="comment"># buffer.memory要大于batch.size，否则会报申请内存不足的错误，不要超过物理内存，根据实际情况调整</span></span><br><span class="line"><span class="attr">buffer.memory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># key的序列化器，将用户提供的 key和value对象ProducerRecord 进行序列化处理，key.serializer必须被设置，即使</span></span><br><span class="line"><span class="comment">#消息中没有指定key，序列化器必须是一个实现org.apache.kafka.common.serialization.Serializer接口的类，将#key序列化成字节数组。</span></span><br><span class="line"><span class="attr">key.serializer</span></span><br><span class="line"><span class="attr">value.serializer</span></span><br></pre></td></tr></table></figure>

<h2 id="Kafka核心API模块-producer-API实战"><a href="#Kafka核心API模块-producer-API实战" class="headerlink" title="Kafka核心API模块-producer API实战"></a>Kafka核心API模块-producer API实战</h2><p>封装配置属性</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">getProperties</span><span class="params">()</span></span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"101.132.252.118:9092"</span>);</span><br><span class="line">        <span class="comment">//props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "101.132.252.118:9092");</span></span><br><span class="line">        <span class="comment">// 当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别,分别是0, 1，all。</span></span><br><span class="line">        props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">        <span class="comment">//props.put(ProducerConfig.ACKS_CONFIG, "all");</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 请求失败，生产者会自动重试，指定是0次，如果启用重试，则会有重复消息的可能性</span></span><br><span class="line">        props.put(<span class="string">"retries"</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="comment">//props.put(ProducerConfig.RETRIES_CONFIG, 0);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 生产者缓存每个分区未发送的消息,缓存的大小是通过 batch.size 配置指定的，默认值是16KB</span></span><br><span class="line">        props.put(<span class="string">"batch.size"</span>, <span class="number">16384</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 默认值就是0，消息是立刻发送的，即便batch.size缓冲空间还没有满</span></span><br><span class="line"><span class="comment">         * 如果想减少请求的数量，可以设置 linger.ms 大于0，即消息在缓冲区保留的时间，超过设置的值就会被提交到服务端</span></span><br><span class="line"><span class="comment">         * 通俗解释是，本该早就发出去的消息被迫至少等待了linger.ms时间，相对于这时间内积累了更多消息，批量发送减少请求</span></span><br><span class="line"><span class="comment">         * 如果batch被填满或者linger.ms达到上限，满足其中一个就会被发送</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.put(<span class="string">"linger.ms"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * buffer.memory的用来约束Kafka Producer能够使用的内存缓冲的大小的，默认值32MB。</span></span><br><span class="line"><span class="comment">         * 如果buffer.memory设置的太小，可能导致消息快速的写入内存缓冲里，但Sender线程来不及把消息发送到Kafka服务器</span></span><br><span class="line"><span class="comment">         * 会造成内存缓冲很快就被写满，而一旦被写满，就会阻塞用户线程，不让继续往Kafka写消息了</span></span><br><span class="line"><span class="comment">         * buffer.memory要大于batch.size，否则会报申请内存不#足的错误，不要超过物理内存，根据实际情况调整</span></span><br><span class="line"><span class="comment">         * 需要结合实际业务情况压测进行配置</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.put(<span class="string">"buffer.memory"</span>, <span class="number">33554432</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * key的序列化器，将用户提供的 key和value对象ProducerRecord 进行序列化处理，key.serializer必须被设置，</span></span><br><span class="line"><span class="comment">         * 即使消息中没有指定key，序列化器必须是一个实现org.apache.kafka.common.serialization.Serializer接口的类，</span></span><br><span class="line"><span class="comment">         * 将key序列化成字节数组。</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>生产者投递消息API实战（同步发送）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * send()方法是异步的，添加消息到缓冲区等待发送，并立即返回</span></span><br><span class="line"><span class="comment">     * 生产者将单个的消息批量在一起发送来提高效率,即 batch.size和linger.ms结合</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 实现同步发送：一条消息发送之后，会阻塞当前线程，直至返回 ack</span></span><br><span class="line"><span class="comment">     * 发送消息后返回的一个 Future 对象，调用get即可</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 消息发送主要是两个线程：一个是Main用户主线程，一个是Sender线程</span></span><br><span class="line"><span class="comment">     *  1)main线程发送消息到RecordAccumulator即返回</span></span><br><span class="line"><span class="comment">     *  2)sender线程从RecordAccumulator拉取信息发送到broker</span></span><br><span class="line"><span class="comment">     *  3)batch.size和linger.ms两个参数可以影响 sender 线程发送次数</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSend</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">3</span>; i++)&#123;</span><br><span class="line">        Future&lt;RecordMetadata&gt;  future = producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my-topic"</span>, <span class="string">"xdclass-key"</span>+i, <span class="string">"xdclass-value"</span>+i));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            RecordMetadata recordMetadata = future.get();<span class="comment">//不关心是否发送成功，则不需要这行</span></span><br><span class="line">            System.out.println(<span class="string">"发送状态："</span>+recordMetadata.toString());</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(i+<span class="string">"发送："</span>+LocalDateTime.now().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="【面试】-ProducerRecord介绍和key的作用"><a href="#【面试】-ProducerRecord介绍和key的作用" class="headerlink" title="【面试】 ProducerRecord介绍和key的作用"></a>【面试】 ProducerRecord介绍和key的作用</h2><p>ProducerRecord（简称PR）：发送给Kafka Broker的key/value 值对, 封装基础数据信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- Topic （名字）</span><br><span class="line">-- PartitionID (可选)</span><br><span class="line">-- Key(可选)</span><br><span class="line">-- Value</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613162107.png" alt="image-20210613162104977"></p>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613162119.png" alt="image-20210613162116729"></p>
<p>key默认是null，大多数应用程序会用到key</p>
<ul>
<li>如果key为空，kafka使用默认的partitioner，使用RoundRobin算法将消息均衡地分布在各个partition上</li>
<li>如果key不为空，kafka使用自己实现的hash方法对key进行散列，决定消息该被写到Topic的哪个partition，拥有相同key的消息会被写到同一个partition，实现顺序消息</li>
</ul>
<h2 id="Kafka核心API模块-producerAPI回调函数实战"><a href="#Kafka核心API模块-producerAPI回调函数实战" class="headerlink" title="Kafka核心API模块-producerAPI回调函数实战"></a>Kafka核心API模块-producerAPI回调函数实战</h2><ul>
<li>生产者发送消息是异步调用，怎么知道是否有异常？<ul>
<li>发送消息配置回调函数即可， 该回调方法会在 Producer 收到 ack 时被调用，为异步调用</li>
<li>回调函数有两个参数 RecordMetadata 和 Exception，如果 Exception 是 null，则消息发送成功，否则失败</li>
</ul>
</li>
<li>异步发送配置回调函数</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSendWithCallback</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">3</span>; i++)&#123;</span><br><span class="line">        producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my-topic"</span>, <span class="string">"xdclass-key"</span> + i, <span class="string">"xdclass-value"</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"发送状态："</span>+metadata.toString());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    exception.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(i+<span class="string">"发送："</span>+LocalDateTime.now().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="producer生产者发送指定分区实战"><a href="#producer生产者发送指定分区实战" class="headerlink" title="producer生产者发送指定分区实战"></a>producer生产者发送指定分区实战</h2><p>创建topic，配置5个分区，1个副本</p>
<p>发送代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSendWithCallbackAndPartition</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; i++)&#123;</span><br><span class="line">        producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"my-topic"</span>,i, <span class="string">"xdclass-key"</span> + i, <span class="string">"xdclass-value"</span> + i), <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"发送状态："</span>+metadata.toString());</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    exception.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(i+<span class="string">"发送："</span>+LocalDateTime.now().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Kafka生产者自定义partition分区规则实战"><a href="#Kafka生产者自定义partition分区规则实战" class="headerlink" title="Kafka生产者自定义partition分区规则实战"></a>Kafka生产者自定义partition分区规则实战</h2><p>源码解读默认分区器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.clients.producer.internals.DefaultPartitioner</span><br></pre></td></tr></table></figure>

<p>自定义分区规则</p>
<ul>
<li>创建类，实现Partitioner接口，重写方法</li>
<li>配置 partitioner.class 指定类即可</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">XdclassPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//返回编号为0的分区</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="string">"xdclass"</span>.equals(key)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//使用hash值取模，确定分区(默认的也是这个方式)</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testSend</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    <span class="comment">//自定义partition分区规则</span></span><br><span class="line">    props.put(<span class="string">"partitioner.class"</span>, <span class="string">"net.xdclass.xdclassredis.XdclassPartitioner"</span>);</span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i++)&#123;</span><br><span class="line">        Future&lt;RecordMetadata&gt;  future = producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(TOPIC_NAME, <span class="string">"xdclass"</span>, <span class="string">"xdclass-value"</span>+i));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            RecordMetadata recordMetadata = future.get();</span><br><span class="line">            System.out.println(<span class="string">"发送状态："</span>+recordMetadata.toString());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(i+<span class="string">"发送："</span>+LocalDateTime.now().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Kafka核心API消费者模块实战"><a href="#Kafka核心API消费者模块实战" class="headerlink" title="Kafka核心API消费者模块实战"></a>Kafka核心API消费者模块实战</h1><h2 id="【面试】Consumer消费者机制和分区策略"><a href="#【面试】Consumer消费者机制和分区策略" class="headerlink" title="【面试】Consumer消费者机制和分区策略"></a>【面试】Consumer消费者机制和分区策略</h2><p>消费者根据什么模式从broker获取数据的？</p>
<p>为什么是pull模式，而不是broker主动push？</p>
<ul>
<li>消费者采用 pull 拉取方式，从broker的partition获取数据</li>
<li>pull 模式则可以根据 consumer 的消费能力进行自己调整，不同的消费者性能不一样<ul>
<li>如果broker没有数据，consumer可以配置 timeout 时间，阻塞等待一段时间之后再返回</li>
</ul>
</li>
<li>如果是broker主动push，优点是可以快速处理消息，但是容易造成消费者处理不过来，消息堆积和延迟。</li>
</ul>
<p>消费者从哪个分区进行消费？</p>
<ul>
<li>一个 topic 有多个 partition，一个消费者组里面有多个消费者，那是怎么分配?<ul>
<li>一个主题topic可以有多个消费者，因为里面有多个partition分区 ( leader分区)</li>
<li>一个partition leader可以由一个消费者组中的一个消费者进行消费</li>
<li>一个 topic 有多个 partition，所以有多个partition leader，给多个消费者消费，那分配策略如何？</li>
</ul>
</li>
</ul>
<p>消费者从哪个分区进行消费？两个策略</p>
<ul>
<li>顶层接口</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.clients.consumer.internals.AbstractPartitionAssignor</span><br></pre></td></tr></table></figure>

<p>策略一：round-robin （<strong>RoundRobinAssignor</strong>非默认策略）轮训</p>
<ul>
<li>【按照消费者组】进行轮训分配，同个消费者组监听不同主题也一样，是把所有的 partition 和所有的 consumer 都列出来， 所以消费者组里面订阅的主题是一样的才行，主题不一样则会出现分配不均问题，例如7个分区，同组内2个消费者</li>
<li>topic-p0/topic-p1/topic-p2/topic-p3/topic-p4/topic-p5/topic-p6</li>
<li>c-1: topic-p0/topic-p2/topic-p4/topic-p6</li>
<li>c-2:topic-p1/topic-p3/topic-p5</li>
<li>弊端<ul>
<li>如果同一消费者组内，所订阅的消息是不相同的，在执行分区分配的时候不是轮询分配，可能会导致分区分配的不均匀</li>
<li>有3个消费者C0、C1和C2，他们共订阅了 3 个主题：t0、t1 和 t2</li>
<li>t0有1个分区(p0)，t1有2个分区(p0、p1)，t2有3个分区(p0、p1、p2))</li>
<li>消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613163952.png" alt="image-20210613163948757"></p>
<p>策略二：range （<strong>RangeAssignor</strong>默认策略）范围</p>
<ul>
<li>【按照主题】进行分配，如果不平均分配，则第一个消费者会分配比较多分区， 一个消费者监听不同主题也不影响，例如7个分区，同组内2个消费者</li>
<li>topic-p0/topic-p1/topic-p2/topic-p3/topic-p4/topic-p5//topic-p6</li>
<li>c-1: topic-p0/topic-p1/topic-p2/topic-p3</li>
<li>c-2:topic-p4/topic-p5/topic-p6</li>
<li>弊端<ul>
<li>只是针对 1 个 topic 而言，c-1多消费一个分区影响不大</li>
<li>如果有 N 多个 topic，那么针对每个 topic，消费者 C-1 都将多消费 1 个分区，topic越多则消费的分区也越多，则性能有所下降</li>
</ul>
</li>
</ul>
<h2 id="【面试】Consumer重新分配策略和offset维护机制"><a href="#【面试】Consumer重新分配策略和offset维护机制" class="headerlink" title="【面试】Consumer重新分配策略和offset维护机制"></a>【面试】Consumer重新分配策略和offset维护机制</h2><p>什么是Rebalance操作？</p>
<ul>
<li>kafka 怎么均匀地分配某个 topic 下的所有 partition 到各个消费者，从而使得消息的消费速度达到最快，这就是平衡（balance），前面讲了 Range 范围分区 和 RoundRobin 轮询分区，也支持自定义分区策略。</li>
<li>而 rebalance（重平衡）其实就是重新进行 partition 的分配，从而使得 partition 的分配重新达到平衡状态</li>
</ul>
<p>面试：例如70个分区，10个消费者，但是先启动一个消费者，后续再启动一个消费者，这个会怎么分配？</p>
<ul>
<li>Kafka 会进行一次分区分配操作，即 Kafka 消费者端的 Rebalance 操作 ，下面都会发生rebalance操作<ul>
<li>当消费者组内的消费者数量发生变化（增加或者减少），就会产生重新分配patition</li>
<li>分区数量发生变化时(即 topic 的分区数量发生变化时)</li>
</ul>
</li>
</ul>
<p>面试：当消费者在消费过程突然宕机了，重新恢复后是从哪里消费，会有什么问题？</p>
<ul>
<li>消费者会记录offset，故障恢复后从这里继续消费，这个offset记录在哪里？</li>
<li>记录在zk里面和本地，新版默认将offset保证在kafka的内置topic中，名称是 __consumer_offsets<ul>
<li>该Topic默认有50个Partition，每个Partition有3个副本，分区数量由参数offset.topic.num.partition配置</li>
<li>通过groupId的哈希值和该参数取模的方式来确定某个消费者组已消费的offset保存到__consumer_offsets主题的哪个分区中</li>
<li>由 消费者组名+主题+分区，确定唯一的offset的key，从而获取对应的值</li>
<li>三元组：<strong>group.id+topic+分区号</strong>，而 value 就是 offset 的值</li>
</ul>
</li>
</ul>
<h2 id="Consumer配置和Kafka调试日志配置"><a href="#Consumer配置和Kafka调试日志配置" class="headerlink" title="Consumer配置和Kafka调试日志配置"></a>Consumer配置和Kafka调试日志配置</h2><p>springboot关闭kafka调试日志</p>
<ol>
<li>yml配置文件修改</li>
</ol>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">logging:</span></span><br><span class="line">  <span class="attr">config:</span> <span class="string">classpath:logback.xml</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>logback.xml内容</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"STDOUT"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 格式化输出： %d表示日期， %thread表示线程名， %-5level: 级别从左显示5个字符宽度 %msg:日志消息, %n是换行符 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;[%thread] %-5level %logger&#123;50&#125; - %msg%n<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"info"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"STDOUT"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>消费者配置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#消费者分组ID，分组内的消费者只能消费该消息一次，不同分组内的消费者可以重复消费该消息</span></span><br><span class="line"><span class="attr">group.id</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#为true则自动提交偏移量</span></span><br><span class="line"><span class="attr">enable.auto.commit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自动提交offset周期</span></span><br><span class="line"><span class="attr">auto.commit.interval.ms</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重置消费偏移量策略，消费者在读取一个没有偏移量的分区或者偏移量无效情况下（因消费者长时间失效、包含偏移量的记录已经过时并被删除）该如何处理，</span></span><br><span class="line"><span class="comment">#默认是latest，如果需要从头消费partition消息，需要改为 earliest 且消费者组名变更 才可以</span></span><br><span class="line"><span class="attr">auto.offset.reset</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#序列化器</span></span><br><span class="line"><span class="attr">key.deserializer</span></span><br></pre></td></tr></table></figure>

<h2 id="Kafka消费者Consumer消费消息配置实战"><a href="#Kafka消费者Consumer消费消息配置实战" class="headerlink" title="Kafka消费者Consumer消费消息配置实战"></a>Kafka消费者Consumer消费消息配置实战</h2><p>配置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">getProperties</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//broker地址</span></span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"101.132.252.118:9092"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//消费者分组ID，分组内的消费者只能消费该消息一次，不同分组内的消费者可以重复消费该消息</span></span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"xdclass-g1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//开启自动提交offset</span></span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自动提交offset延迟时间</span></span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//反序列化</span></span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>消费订阅</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">simpleConsumerTest</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//订阅topic主题</span></span><br><span class="line">    consumer.subscribe(Arrays.asList(KafkaProducerTest.TOPIC_NAME));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="comment">//拉取时间控制，阻塞超时时间</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">500</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.err.printf(<span class="string">"topic = %s, offset = %d, key = %s, value = %s%n"</span>,record.topic(), record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Consumer从头消费配置和手工提交offset配置"><a href="#Consumer从头消费配置和手工提交offset配置" class="headerlink" title="Consumer从头消费配置和手工提交offset配置"></a>Consumer从头消费配置和手工提交offset配置</h2><p>如果需要从头消费partition消息，怎么操作？</p>
<ul>
<li>auto.offset.reset 配置策略即可</li>
<li>默认是latest，需要改为 earliest 且消费者组名变更 ，即可实现从头消费</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//默认是latest，如果需要从头消费partition消息，需要改为 earliest 且消费者组名变更，才生效 </span></span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>,<span class="string">"earliest"</span>);</span><br></pre></td></tr></table></figure>

<p>自动提交offset问题</p>
<ul>
<li>没法控制消息是否正常被消费</li>
<li>适合非严谨的场景，比如日志收集发送</li>
</ul>
<p>手工提交offset配置和测试</p>
<ul>
<li>初次启动消费者会请求broker获取当前消费的offset值</li>
</ul>
<p>手工提交offset</p>
<ul>
<li>同步 commitSync 阻塞当前线程 (自动失败重试）</li>
<li>异步 commitAsync 不会阻塞当前线程 (没有失败重试，回调callback函数获取提交信息，记录日志)</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">simpleConsumerTest</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Properties props = getProperties();</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//订阅topic主题</span></span><br><span class="line">    consumer.subscribe(Arrays.asList(KafkaProducerTest.TOPIC_NAME));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="comment">//拉取时间控制，阻塞超时时间</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">500</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.err.printf(<span class="string">"topic = %s, offset = %d, key = %s, value = %s%n"</span>,record.topic(), record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(!records.isEmpty())&#123;</span><br><span class="line">            <span class="comment">//异步 commitAsync 手工提交</span></span><br><span class="line">            consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">if</span>(exception == <span class="keyword">null</span>)&#123;</span><br><span class="line">                        System.err.println(<span class="string">"手工提交offset成功"</span>+offsets.toString());</span><br><span class="line">                    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                        System.err.println(<span class="string">"手工提交offset失败"</span>+offsets.toString());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="kafka数据文件存储-可靠性保证-ISR核心知识"><a href="#kafka数据文件存储-可靠性保证-ISR核心知识" class="headerlink" title="kafka数据文件存储-可靠性保证-ISR核心知识"></a>kafka数据文件存储-可靠性保证-ISR核心知识</h1><h2 id="Kafka数据存储流程和log日志"><a href="#Kafka数据存储流程和log日志" class="headerlink" title="Kafka数据存储流程和log日志"></a>Kafka数据存储流程和log日志</h2><ul>
<li>Kafka 采取了<strong>分片</strong>和<strong>索引</strong>机制，将每个partition分为多个segment，每个segment对应2个文件 log 和 index</li>
<li>新增备注</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index文件中并没有为每一条message建立索引，采用了稀疏存储的方式</span><br><span class="line">每隔一定字节的数据建立一条索引，避免了索引文件占用过多的空间和资源，从而可以将索引文件保留到内存中</span><br><span class="line">缺点是没有建立索引的数据在查询的过程中需要小范围内的顺序扫描操作。</span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613165726.png" alt="image-20210613165723685"></p>
<p>配置文件 server.properties</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The maximum size of a log segment file. When this size is reached a new log segment will be created. 默认是1G,当log数据文件大于1g后，会创建一个新的log文件（即segment，包括index和log）</span></span><br><span class="line"><span class="meta">log.segment.bytes</span>=<span class="string">1073741824</span></span><br></pre></td></tr></table></figure>

<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613170743.png" alt="image-20210613170738940"></p>
<p>例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#分段一</span><br><span class="line">00000000000000000000.index  00000000000000000000.log</span><br><span class="line">#分段二 数字 1234指的是当前文件的最小偏移量offset，即上个文件的最后一个消息的offset+1</span><br><span class="line">00000000000000001234.index  00000000000000001234.log</span><br><span class="line">#分段三</span><br><span class="line">00000000000000088888.index  00000000000000088888.log</span><br></pre></td></tr></table></figure>

<h2 id="【核心】分布式系统的CAP理论"><a href="#【核心】分布式系统的CAP理论" class="headerlink" title="【核心】分布式系统的CAP理论"></a>【核心】分布式系统的CAP理论</h2><ul>
<li>CAP定理: 指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可同时获得<ul>
<li>一致性（C）：所有节点都可以访问到最新的数据；锁定其他节点，不一致之前不可读</li>
<li>可用性（A）：每个请求都是可以得到响应的，不管请求是成功还是失败；被节点锁定后 无法响应</li>
<li>分区容错性（P）：除了全部整体网络故障，其他故障都不能导致整个系统不可用,；节点间通信可能失败，无法避免</li>
</ul>
</li>
<li>CAP理论就是说在分布式存储系统中，最多只能实现上面的两点。而由于当前的网络硬件肯定会出现延迟丢包等问题，所以分区容忍性是我们必须需要实现的。所以我们只能在一致性和可用性之间进行权衡</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613170939.png" alt="image-20210613170936248"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CA： 如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，这是违背分布式系统设计的初衷的</span><br><span class="line"></span><br><span class="line">CP: 如果不要求A（可用），每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统</span><br><span class="line"></span><br><span class="line">AP：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。</span><br></pre></td></tr></table></figure>

<p>结论：</p>
<ul>
<li>分布式系统中P,肯定要满足，所以只能在CA中二选一</li>
<li>没有最好的选择，最好的选择是根据业务场景来进行架构设计</li>
<li>CP ： 适合支付、交易类，要求数据强一致性，宁可业务不可用，也不能出现脏数据</li>
<li>AP: 互联网业务，比如信息流架构，不要求数据强一致，更想要服务可用</li>
</ul>
<h2 id="Kafka数据可靠性保证原理之副本Replica-ACK"><a href="#Kafka数据可靠性保证原理之副本Replica-ACK" class="headerlink" title="Kafka数据可靠性保证原理之副本Replica+ACK"></a>Kafka数据可靠性保证原理之副本Replica+ACK</h2><ul>
<li>背景<ul>
<li>Kafka之间副本数据同步是怎样的？一致性怎么保证，数据怎样保证不丢失呢</li>
</ul>
</li>
<li>kafka的副本（replica）<ul>
<li>topic可以设置有N个副本, 副本数最好要小于broker的数量</li>
<li>每个分区有1个leader和0到多个follower，我们把多个replica分为Learder replica和follower replica</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613171206.png" alt="image-20210613171204652"></p>
<p>生产者发送数据流程</p>
<ul>
<li>保证producer 发送到指定的 topic， topic 的每个 partition 收到producer 发送的数据后</li>
<li>需要向 producer 发送 ack 确认收到，如果producer 收到 ack， 就会进行下一轮的发送否则重新发送数据</li>
</ul>
<p>问题点：Partition什么时间发送ack确认机制（要追求高吞吐量，那么就要放弃可靠性）</p>
<p> 当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别</p>
<ul>
<li><p>副本数据同步策略 , ack有3个可选值，分别是0, 1，all。</p>
<ul>
<li><p>ack=0</p>
<ul>
<li>producer发送一次就不再发送了，不管是否发送成功</li>
<li>发送出去的消息还在半路，或者还没写入磁盘， Partition Leader所在Broker就直接挂了，客户端认为消息发送成功了，此时就会导致这条消息就丢失</li>
</ul>
</li>
<li><p>ack=1(默认)</p>
<ul>
<li>只要Partition Leader接收到消息而且写入【本地磁盘】，就认为成功了，不管他其他的Follower有没有同步过去这条消息了</li>
<li>问题：万一Partition Leader刚刚接收到消息，Follower还没来得及同步过去，结果Leader所在的broker宕机了</li>
</ul>
</li>
<li><p>ack= all（即-1）</p>
<ul>
<li><p>producer只有收到分区内所有副本的成功写入全部落盘的通知才认为推送消息成功</p>
</li>
<li><p>备注：leader会维持一个与其保持同步的replica集合，该集合就是ISR，leader副本也在isr里面</p>
</li>
<li><p>问题一：如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复</p>
<ul>
<li>数据发送到leader后 ，部分ISR的副本同步，leader此时挂掉。比如follower1和follower2都有可能变成新的leader, producer端会得到返回异常，producer端会重新发送数据，数据可能会重复</li>
</ul>
</li>
<li><p>问题二：acks=all 就可以代表数据一定不会丢失了吗</p>
<ul>
<li>Partition只有一个副本，也就是一个Leader，任何Follower都没有</li>
<li>接收完消息后宕机，也会导致数据丢失，acks=all，必须跟ISR列表里至少有2个以上的副本配合使用</li>
<li>在设置request.required.acks=-1的同时，也要min.insync.replicas这个参数设定 ISR中的最小副本数是多少，默认值为1，改为 &gt;=2，如果ISR中的副本数少于min.insync.replicas配置的数量时，客户端会返回异常</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Kafka的in-sync-replica-set机制"><a href="#Kafka的in-sync-replica-set机制" class="headerlink" title="Kafka的in-sync replica set机制"></a>Kafka的in-sync replica set机制</h2><ul>
<li>什么是ISR (<strong>in-sync replica set</strong> )<ul>
<li>leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个leader partition都有一个ISR，leader动态维护, 要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功</li>
<li>Partition leader 保持同步的 Partition Follower 集合, 当 ISR 中的Partition Follower 完成数据的同步之后，就会给 leader 发送 ack</li>
<li>如果Partition follower长时间(replica.lag.time.max.ms) 未向leader同步数据，则该Partition Follower将被踢出ISR</li>
<li>Partition Leader 发生故障之后，就会从 ISR 中选举新的 Partition Leader。</li>
</ul>
</li>
<li>OSR （out-of-sync-replica set）：与leader副本分区 同步滞后过多的副本集合</li>
<li>AR（Assign Replicas）：分区中所有副本统称为AR</li>
</ul>
<h2 id="Kafka的HighWatermark的作用"><a href="#Kafka的HighWatermark的作用" class="headerlink" title="Kafka的HighWatermark的作用"></a>Kafka的HighWatermark的作用</h2><ul>
<li><p>背景 broker故障后</p>
<ul>
<li>ACK保障了【生产者】的投递可靠性</li>
<li>partition的多副本保障了【消息存储】的可靠性</li>
<li>备注：重复消费问题需要消费者自己处理</li>
</ul>
</li>
<li><p>HW作用：保证消费数据的一致性和副本数据的一致性</p>
</li>
<li><p>Follower故障</p>
<ul>
<li>Follower发生故障后会被临时踢出ISR（动态变化），待该follower恢复后，follower会读取本地的磁盘记录的上次的HW，并将该log文件高于HW的部分截取掉，从HW开始向leader进行同步，等该follower的LEO大于等于该Partition的hw，即follower追上leader后，就可以重新加入ISR</li>
</ul>
</li>
<li><p>Leader故障</p>
<ul>
<li>Leader发生故障后，会从ISR中选出一个新的leader，为了保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于hw的部分截掉（新leader自己不会截掉），然后从新的leader同步数据</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613171903.png" alt="image-20210613171900754"></p>
<h1 id="kafka高可用集群和高性能"><a href="#kafka高可用集群和高性能" class="headerlink" title="kafka高可用集群和高性能"></a>kafka高可用集群和高性能</h1><h2 id="Kafka高可用集群搭建节点需求规划"><a href="#Kafka高可用集群搭建节点需求规划" class="headerlink" title="Kafka高可用集群搭建节点需求规划"></a>Kafka高可用集群搭建节点需求规划</h2><ul>
<li>注意<ul>
<li>没那么多机器，采用伪集群方式搭建（端口号区分）</li>
<li>zookeeper部署3个节点<ul>
<li>2181</li>
<li>2182</li>
<li>2183</li>
</ul>
</li>
<li>kafka部署3个节点<ul>
<li>9092</li>
<li>9093</li>
<li>9094</li>
</ul>
</li>
</ul>
</li>
<li>网络安全组记得开放端口</li>
</ul>
<h2 id="Kafka-ZooKeeper"><a href="#Kafka-ZooKeeper" class="headerlink" title="Kafka + ZooKeeper"></a>Kafka + ZooKeeper</h2><p>ZooKeeper 的官网是：<a href="https://zookeeper.apache.org/。在官网上是这么介绍" target="_blank" rel="noopener">https://zookeeper.apache.org/。在官网上是这么介绍</a> ZooKeeper 的：ZooKeeper 是一项集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。当我们编写程序的时候，通常会将所有的配置信息保存在一个配置文件中，例如账号、密码等信息，后续直接修改配置文件就行了，那分布式场景下如何配置呢？如果说每台机器上都保存一个配置文件，这时候要一台台的去修改配置文件难免出错，而且要管理这些机器也会变得复杂和困难，ZooKeeper 的出现就是为了解决这类问题，实现高度可靠的分布式系统。</p>
<ol>
<li><p><strong>配置管理</strong>：ZooKeeper 为分布式系统提供了一种配置管理的服务：集中管理配置，即将全局配置信息保存在 ZooKeeper 服务中，方便进行修改和管理，省去了手动拷贝配置的过程，同时还保证了可靠和一致性。</p>
</li>
<li><p><strong>命名服务</strong>：在分布式系统中，经常需要对应用或者服务进行统一命名，便于识别和区分开来，而 ZooKeeper 就提供了这种服务。</p>
</li>
<li><p><strong>分布式锁</strong>：</p>
<p>　锁应该都不陌生，没有用过也听说过，在多个进程访问互斥资源的时候，需要加上一道锁。在分布式系统中，分布式程序分布在各个主机上的进程对互斥资源进行访问时也需要加锁。</p>
<p>　　分布式锁应当具备以下条件：</p>
<ul>
<li>在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行；</li>
<li>高可用的获取锁与释放锁；</li>
<li>高性能的获取锁与释放锁；</li>
<li>具备可重入特性（可理解为重新进入，由多于一个任务并发使用，而不必担心数据错误）；</li>
<li>具备锁失效机制，防止死锁；</li>
<li>具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。</li>
</ul>
</li>
<li><p><strong>集群管理</strong>：　在分布式系统中，由于各种各样的原因，例如机器故障、网络故障等，导致集群中的节点增加或者减少，集群中有些机器需要感知到这种变化，然后根据这种变化做出对应的决策。</p>
</li>
<li><p><strong>基本架构</strong>：</p>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613172615.png" alt="image-20210613172612357"></p>
</li>
</ol>
<p><strong>Kafka + ZooKeeper</strong></p>
<p>ZooKeeper 作为给分布式系统提供协调服务的工具被 kafka 所依赖。在分布式系统中，消费者需要知道有哪些生产者是可用的，而如果每次消费者都需要和生产者建立连接并测试是否成功连接，那效率也太低了，显然是不可取的。而通过使用 ZooKeeper 协调服务，Kafka 就能将 Producer，Consumer，Broker 等结合在一起，同时借助 ZooKeeper，Kafka 就能够将所有组件在无状态的条件下建立起生产者和消费者的订阅关系，实现负载均衡。</p>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613174100.png" alt="image-20210613174057292"></p>
<p><strong>Broker 信息</strong></p>
<p>　　在 ZooKeeper 上会有一个专门用来进行 Broker 服务器列表记录的节点，节点路径为 /brokers/ids。Kafka 的每个 Broker 启动时，都会在 ZooKeeper 中注册，创建 /brokers/ids/[0-N] 节点，写入 IP，端口等信息，每个 Broker 都有一个 BrokerId。Broker 创建的是临时节点，在连接断开时节点就会自动删除，所以在 ZooKeeper 上就可以通过 Broker 中节点的变化来得到 Broker 的可用性。</p>
<p><strong>Topic 信息</strong></p>
<p>　　在 Kafka 中可以定义很多个 Topic，每个 Topic 又被分为很多个 Partition。一般情况下，每个 Partition 独立在存在一个 Broker 上，所有的这些 Topic 和 Broker 的对应关系都由 ZooKeeper 进行维护。</p>
<p><strong>负载均衡</strong></p>
<p>　　生产者需要将消息发送给 Broker，消费者需要从 Broker 上获取消息，通过使用 ZooKeeper，就都能监听 Broker 上节点的状态信息，从而实现动态负载均衡。</p>
<p><strong>offset 信息</strong></p>
<p>　　offset 用于记录消费者消费到的位置，在老版本（0.9以前）里 offset 是保存在 ZooKeeper 中的。</p>
<p><strong>Controller 选举</strong></p>
<p>　　在 Kafka 中会有多个 Broker，其中一个 Broker 会被选举成为 Controller（控制器），在任意时刻，Kafka 集群中有且仅有一个控制器。Controller负责管理集群中所有分区和副本的状态，当某个分区的 leader 副本出现故障时，由 Controller 为该分区选举出一个新的 leader。Kafka 的 Controller 选举就依靠 ZooKeeper 来完成，成功竞选为 Controller 的 Broker 会在 ZooKeeper 中创建 /controller 这个临时节点，在 ZooKeeper 中使用 get 命令查看节点内容：</p>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613174655.png" alt="image-20210613174645754"></p>
<p>其中“version”在目前版本中固定为1，“brokerid”表示 Broker 的编号，“timestamp”表示竞选称为 Controller 时的时间戳。</p>
<p>当 Broker 启动时，会尝试读取 /controller 中的“brokerid”，如果读取到的值不是-1，则表示已经有节点竞选成为 Controller 了，当前节点就会放弃竞选；而如果读取到的值为-1，ZooKeeper 就会尝试创建 /controller 节点，当该 Broker 去创建的时候，可能还有其他 Broker 一起同时创建节点，但只有一个 Broker 能够创建成功，即成为唯一的 Controller。</p>
<h2 id="Kafka高可用集群之zookeeper集群环境准备"><a href="#Kafka高可用集群之zookeeper集群环境准备" class="headerlink" title="Kafka高可用集群之zookeeper集群环境准备"></a>Kafka高可用集群之zookeeper集群环境准备</h2><ul>
<li><p>cp -r 复制zk节点，一共3个</p>
</li>
<li><p>修改配置zoo.cfg</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#客户端端口，三个客户端端口分别为2181 2182 2183</span></span><br><span class="line"><span class="attr">clientPort</span>=<span class="string">2181</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据存储路径，/tmp/zookeeper/2181 /tmp/zookeeper/2182 /tmp/zookeeper/2183</span></span><br><span class="line"><span class="attr">dataDir</span>=<span class="string">/tmp/zookeeper/2181</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改AdminServer的端口：8888 8889 8890</span></span><br><span class="line"><span class="meta">admin.serverPort</span>=<span class="string">8888</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>dataDir对应目录下分别创建myid文件，内容对应1、2、3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tmp/zookeeper/2181</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; myid</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置集群</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># server.服务器id=服务器IP地址:服务器直接通信端口:服务器之间选举投票端口</span></span><br><span class="line"></span><br><span class="line"><span class="meta">server.1</span>=<span class="string">127.0.0.1:2881:3881</span></span><br><span class="line"><span class="meta">server.2</span>=<span class="string">127.0.0.1:2882:3882</span></span><br><span class="line"><span class="meta">server.3</span>=<span class="string">127.0.0.1:2883:3883</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>zk命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#启动zk</span></span><br><span class="line">./zkServer.sh  start</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看节点状态</span></span><br><span class="line">./zkServer.sh status</span><br><span class="line"></span><br><span class="line"><span class="comment">#停止节点</span></span><br><span class="line">./zkServer.sh stop</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Kafka高可用集群搭建-环境准备"><a href="#Kafka高可用集群搭建-环境准备" class="headerlink" title="Kafka高可用集群搭建-环境准备"></a>Kafka高可用集群搭建-环境准备</h2><ul>
<li><p>伪集群搭建，3个节点同个机器端口区分</p>
<ul>
<li>9092</li>
<li>9093</li>
<li>9094</li>
</ul>
</li>
<li><p>配置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#内网中使用，内网部署 kafka 集群只需要用到 listeners，内外网需要作区分时 才需要用到advertised.listeners</span></span><br><span class="line"><span class="attr">listeners</span>=<span class="string">PLAINTEXT://172.23.148.108:9092</span></span><br><span class="line"></span><br><span class="line"><span class="meta">advertised.listeners</span>=<span class="string">PLAINTEXT://101.132.252.118:9092</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#每个节点编号1、2、3</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#端口</span></span><br><span class="line"><span class="attr">port</span>=<span class="string">9092</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置3个</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/tmp/kafka-logs-1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#zk地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">localhost:2181,localhost:2182,localhost:2183</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h1 id="Kafka高可用集群搭建实战-SpringBoot项目测试"><a href="#Kafka高可用集群搭建实战-SpringBoot项目测试" class="headerlink" title="Kafka高可用集群搭建实战+SpringBoot项目测试"></a>Kafka高可用集群搭建实战+SpringBoot项目测试</h1><p>启动Kafka实战</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 守护进程</span></span><br><span class="line">./kafka-server-start.sh -daemon ../config/server.properties &amp;</span><br><span class="line"></span><br><span class="line">./kafka-server-start.sh ../config/server.properties &amp;</span><br></pre></td></tr></table></figure>

<p>创建topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --create --zookeeper 101.132.252.118:2181,101.132.252.118:2182,101.132.252.118:2183 --replication-factor 3 --partitions 6 --topic xdclass-cluster-topic</span><br></pre></td></tr></table></figure>

<p>SpringBoot项目测试</p>
<ul>
<li>连接zookeeper集群</li>
<li>创建topic</li>
<li>查看topic详情</li>
<li>发送消息</li>
</ul>
<h2 id="Kafka的中的日志数据清理"><a href="#Kafka的中的日志数据清理" class="headerlink" title="Kafka的中的日志数据清理"></a>Kafka的中的日志数据清理</h2><ul>
<li><p>Kafka将数据持久化到了硬盘上，为了控制磁盘容量，需要对过去的消息进行清理</p>
</li>
<li><p>问题：如果让你去设计这个日志删除策略，你会怎么设计？【原理思想】很重要的体现，下面是kafka答案</p>
<ul>
<li>内部有个定时任务检测删除日志，默认是5分钟 log.retention.check.interval.ms</li>
<li>支持配置策略对数据清理</li>
<li>根据segment单位进行定期清理</li>
</ul>
</li>
<li><p>启用cleaner</p>
<ul>
<li>log.cleaner.enable=true</li>
<li>log.cleaner.threads = 2 (清理线程数配置)</li>
</ul>
</li>
<li><p>日志删除</p>
<ul>
<li><p>log.cleanup.policy=delete</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#清理超过指定时间的消息,默认是168小时，7天</span></span><br><span class="line"><span class="comment">#还有log.retention.ms, log.retention.minutes, log.retention.hours，优先级高到低</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#超过指定大小后，删除旧的消息，下面是1G的字节数，-1就是没限制</span></span><br><span class="line"><span class="meta">log.retention.bytes</span>=<span class="string">1073741824</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#还有基于日志起始位移（log start offset)，未来社区还有更多</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>基于【时间删除】 日志说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">配置了7天后删除，那7天如何确定呢？</span><br><span class="line"></span><br><span class="line">每个日志段文件都维护一个最大时间戳字段，每次日志段写入新的消息时，都会更新该字段</span><br><span class="line"></span><br><span class="line">一个日志段segment写满了被切分之后，就不再接收任何新的消息，最大时间戳字段的值也将保持不变</span><br><span class="line"></span><br><span class="line">kafka通过将当前时间与该最大时间戳字段进行比较，从而来判定是否过期</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于【大小超过阈值】 删除日志 说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">假设日志段大小是500MB，当前分区共有4个日志段文件，大小分别是500MB，500MB，500MB和10MB</span><br><span class="line"></span><br><span class="line">10MB那个文件就是active日志段。</span><br><span class="line"></span><br><span class="line">此时该分区总的日志大小是3*500MB+10MB&#x3D;1500MB+10MB</span><br><span class="line"></span><br><span class="line">如果阈值设置为1500MB，那么超出阈值的部分就是10MB，小于日志段大小500MB，故Kafka不会执行任何删除操作，即使总大小已经超过了阈值；</span><br><span class="line"></span><br><span class="line">如果阈值设置为1000MB，那么超过阈值的部分就是500MB+10MB &gt; 500MB，此时Kafka会删除最老的那个日志段文件</span><br><span class="line"></span><br><span class="line">注意：超过阈值的部分必须要大于一个日志段的大小</span><br></pre></td></tr></table></figure>
</li>
<li><p>log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除</p>
</li>
<li><p>日志压缩</p>
<ul>
<li>log.cleanup.policy=compact 启用压缩策略</li>
<li>按照消息key进行整理，有相同key不同value值，只保留最后一个</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Kafka的高性能原理分析-ZeroCopy"><a href="#Kafka的高性能原理分析-ZeroCopy" class="headerlink" title="Kafka的高性能原理分析-ZeroCopy"></a>Kafka的高性能原理分析-ZeroCopy</h2><p>零拷贝ZeroCopy（SendFile）</p>
<ul>
<li>例子：将一个File读取并发送出去（Linux有两个上下文，内核态，用户态）<ul>
<li>File文件的经历了4次copy<ul>
<li>调用read,将文件拷贝到了kernel内核态</li>
<li>CPU控制 kernel态的数据copy到用户态</li>
<li>调用write时，user态下的内容会copy到内核态的socket的buffer中</li>
<li>最后将内核态socket buffer的数据copy到网卡设备中传送</li>
</ul>
</li>
<li>缺点：增加了上下文切换、浪费了2次无效拷贝(即步骤2和3)</li>
</ul>
</li>
</ul>
<p><img src="https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210613182530.png" alt="image-20210613182527789"></p>
<ul>
<li>ZeroCopy：<ul>
<li>请求kernel直接把disk的data传输给socket，而不是通过应用程序传输。Zero copy大大提高了应用程序的性能，减少不必要的内核缓冲区跟用户缓冲区间的拷贝，从而减少CPU的开销和减少了kernel和user模式的上下文切换，达到性能的提升</li>
<li>对应零拷贝技术有mmap及sendfile<ul>
<li>mmap:小文件传输快</li>
<li>sendfile:大文件传输比mmap快</li>
</ul>
</li>
<li>应用：Kafka、Netty、RocketMQ等都采用了零拷贝技术</li>
</ul>
</li>
</ul>
<h2 id="Kafka的高性能原理分析归纳总结"><a href="#Kafka的高性能原理分析归纳总结" class="headerlink" title="Kafka的高性能原理分析归纳总结"></a>Kafka的高性能原理分析归纳总结</h2><p>kafka高性能</p>
<ul>
<li>存储模型，topic多分区，每个分区多segment段</li>
<li>index索引文件查找，利用分段和稀疏索引</li>
<li>磁盘顺序写入</li>
<li>异步操作少阻塞sender和main线程，批量操作(batch)</li>
<li>页缓存Page cache，没利用JVM内存，因为容易GC影响性能</li>
<li>零拷贝ZeroCopy（SendFile）</li>
</ul>
<h1 id="SpringBoot项目整合Spring-kafka和事务消息实战"><a href="#SpringBoot项目整合Spring-kafka和事务消息实战" class="headerlink" title="SpringBoot项目整合Spring-kafka和事务消息实战"></a>SpringBoot项目整合Spring-kafka和事务消息实战</h1><h2 id="Springboot项目整合spring-kafka依赖包配置"><a href="#Springboot项目整合spring-kafka依赖包配置" class="headerlink" title="Springboot项目整合spring-kafka依赖包配置"></a>Springboot项目整合spring-kafka依赖包配置</h2><p>添加pom文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置文件修改增加生产者信息</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">101.132</span><span class="number">.252</span><span class="number">.118</span><span class="string">:9092,101.132.252.118:9093,101.132.252.118:9094</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># 消息重发的次数</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">0</span></span><br><span class="line">      <span class="comment"># 一个批次可以使用的内存大小</span></span><br><span class="line">      <span class="attr">batch-size:</span> <span class="number">16384</span></span><br><span class="line">      <span class="comment"># 设置生产者内存缓冲区的大小</span></span><br><span class="line">      <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br><span class="line">      <span class="comment"># 键的序列化方式</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># 值的序列化方式</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="attr">acks:</span> <span class="string">all</span></span><br></pre></td></tr></table></figure>

<p>发送消息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UserController</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span>  <span class="keyword">final</span> String TOPIC_NAME = <span class="string">"user.register.topic"</span>;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, Object&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/api/v1/&#123;num&#125;"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">(@PathVariable(<span class="string">"num"</span>)</span> String num)</span>&#123;</span><br><span class="line"></span><br><span class="line">        kafkaTemplate.send(TOPIC_NAME,<span class="string">"这是一个消息，num="</span>+num).addCallback(success-&gt;&#123;</span><br><span class="line">            String topic = success.getRecordMetadata().topic();</span><br><span class="line">            <span class="keyword">int</span> partition = success.getRecordMetadata().partition();</span><br><span class="line">            <span class="keyword">long</span> offset = success.getRecordMetadata().offset();</span><br><span class="line">            System.out.println(<span class="string">"发送成功：topic="</span>+topic+<span class="string">", partition="</span>+partition+<span class="string">", offset="</span>+offset);</span><br><span class="line">        &#125;,failure-&gt;&#123;</span><br><span class="line">            System.out.println(<span class="string">"发送失败："</span>+failure.getMessage());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Springboot项目整合spring-kafka监听消费消息"><a href="#Springboot项目整合spring-kafka监听消费消息" class="headerlink" title="Springboot项目整合spring-kafka监听消费消息"></a>Springboot项目整合spring-kafka监听消费消息</h2><p>配置文件修改增加消费者信息</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">101.132</span><span class="number">.252</span><span class="number">.118</span><span class="string">:9092,101.132.252.118:9093,101.132.252.118:9094</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># 消息重发的次数。</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment">#一个批次可以使用的内存大小</span></span><br><span class="line">      <span class="attr">batch-size:</span> <span class="number">16384</span></span><br><span class="line">      <span class="comment"># 设置生产者内存缓冲区的大小。</span></span><br><span class="line">      <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br><span class="line">      <span class="comment"># 键的序列化方式</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># 值的序列化方式</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="attr">acks:</span> <span class="string">all</span></span><br><span class="line">      <span class="comment">#事务id</span></span><br><span class="line">      <span class="attr">transaction-id-prefix:</span> <span class="string">xdclass-tran-</span></span><br><span class="line">    <span class="attr">consumer:</span></span><br><span class="line">      <span class="comment"># 自动提交的时间间隔 在spring boot 2.X 版本是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D</span></span><br><span class="line">      <span class="attr">auto-commit-interval:</span> <span class="string">1S</span></span><br><span class="line">      <span class="comment"># 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span></span><br><span class="line">      <span class="attr">auto-offset-reset:</span> <span class="string">earliest</span></span><br><span class="line">      <span class="comment"># 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量</span></span><br><span class="line">      <span class="attr">enable-auto-commit:</span> <span class="literal">false</span></span><br><span class="line">      <span class="comment"># 键的反序列化方式</span></span><br><span class="line">      <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="comment"># 值的反序列化方式</span></span><br><span class="line">      <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">    <span class="attr">listener:</span></span><br><span class="line">      <span class="comment">#手工ack，调用ack后立刻提交offset</span></span><br><span class="line">      <span class="attr">ack-mode:</span> <span class="string">manual_immediate</span></span><br><span class="line">      <span class="comment">#容器运行的线程数</span></span><br><span class="line">      <span class="attr">concurrency:</span> <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>代码编写</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MQListener</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  消费监听</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener</span>(topics = &#123;<span class="string">"user.register.topic"</span>&#125;,groupId = <span class="string">"xdlcass-test-gp"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onMessage1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record, Acknowledgment ack, @Header(KafkaHeaders.RECEIVED_TOPIC)</span> String topic)</span>&#123;</span><br><span class="line">        <span class="comment">// 打印出消息内容</span></span><br><span class="line">        System.out.println(<span class="string">"消费："</span>+record.topic()+<span class="string">"-"</span>+record.partition()+<span class="string">"-"</span>+record.value());</span><br><span class="line">        ack.acknowledge();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Kafka事务消息-整合SpringBoot实战"><a href="#Kafka事务消息-整合SpringBoot实战" class="headerlink" title="Kafka事务消息-整合SpringBoot实战"></a>Kafka事务消息-整合SpringBoot实战</h2><ul>
<li><p>Kafka 从 0.11 版本开始引入了事务支持</p>
<ul>
<li>事务可以保证对多个分区写入操作的原子性</li>
<li>操作的原子性是指多个操作要么全部成功，要么全部失败，不存在部分成功、部分失败的可能</li>
</ul>
</li>
<li><p>配置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring</span>:<span class="string"></span></span><br><span class="line">  <span class="attr">kafka</span>:<span class="string"></span></span><br><span class="line">    <span class="meta">bootstrap-servers</span>: <span class="string">112.74.55.160:9092,112.74.55.160:9093,112.74.55.160:9094</span></span><br><span class="line">    <span class="attr">producer</span>:<span class="string"></span></span><br><span class="line"><span class="comment">      # 消息重发的次数。 配置事务的话：如果用户显式地指定了 retries 参数，那么这个参数的值必须大于0</span></span><br><span class="line"><span class="comment">      #retries: 1</span></span><br><span class="line"><span class="comment">      #一个批次可以使用的内存大小</span></span><br><span class="line">      <span class="meta">batch-size</span>: <span class="string">16384</span></span><br><span class="line"><span class="comment">      # 设置生产者内存缓冲区的大小。</span></span><br><span class="line">      <span class="meta">buffer-memory</span>: <span class="string">33554432</span></span><br><span class="line"><span class="comment">      # 键的序列化方式</span></span><br><span class="line">      <span class="meta">key-serializer</span>: <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="comment">      # 值的序列化方式</span></span><br><span class="line">      <span class="meta">value-serializer</span>: <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="comment">      #配置事务的话：如果用户显式地指定了 acks 参数，那么这个参数的值必须-1 all</span></span><br><span class="line"><span class="comment">      #acks: all</span></span><br><span class="line"></span><br><span class="line"><span class="comment">      #事务id</span></span><br><span class="line">      <span class="meta">transaction-id-prefix</span>: <span class="string">xdclass-tran</span></span><br><span class="line">    <span class="attr">consumer</span>:<span class="string"></span></span><br><span class="line"><span class="comment">      # 自动提交的时间间隔 在spring boot 2.X 版本是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D</span></span><br><span class="line">      <span class="meta">auto-commit-interval</span>: <span class="string">1S</span></span><br><span class="line"></span><br><span class="line"><span class="comment">      # 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span></span><br><span class="line">      <span class="meta">auto-offset-reset</span>: <span class="string">earliest</span></span><br><span class="line"></span><br><span class="line"><span class="comment">      # 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量</span></span><br><span class="line">      <span class="meta">enable-auto-commit</span>: <span class="string">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">      # 键的反序列化方式</span></span><br><span class="line">      <span class="meta">key-deserializer</span>: <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="comment">      # 值的反序列化方式</span></span><br><span class="line">      <span class="meta">value-deserializer</span>: <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">listener</span>:<span class="string"></span></span><br><span class="line"><span class="comment">      # 在侦听器容器中运行的线程数。</span></span><br><span class="line">      <span class="attr">concurrency</span>: <span class="string">4</span></span><br><span class="line"><span class="comment">      #listner负责ack，手动调用Acknowledgment.acknowledge()后立即提交</span></span><br><span class="line">      <span class="meta">ack-mode</span>: <span class="string">manual_immediate</span></span><br><span class="line"><span class="comment">      #避免出现主题未创建报错</span></span><br><span class="line">      <span class="meta">missing-topics-fatal</span>: <span class="string">false</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>SpringBoot代码编写</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注解方式的事务</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> i</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/kafka/transaction1"</span>)</span><br><span class="line">    <span class="meta">@Transactional</span>(rollbackFor = RuntimeException<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">    <span class="title">public</span> <span class="title">void</span> <span class="title">sendMessage1</span>(<span class="title">int</span> <span class="title">i</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">        kafkaTemplate.send(TOPIC_NAME, <span class="string">"这个是事务里面的消息：1  i="</span>+i);</span><br><span class="line">            <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"fail"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        kafkaTemplate.send(TOPIC_NAME, <span class="string">"这个是事务里面的消息：2  i="</span>+i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 声明式事务支持</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> i</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@GetMapping</span>(<span class="string">"/kafka/transaction2"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendMessage2</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">        kafkaTemplate.executeInTransaction(<span class="keyword">new</span> KafkaOperations.OperationsCallback() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Object <span class="title">doInOperations</span><span class="params">(KafkaOperations kafkaOperations)</span> </span>&#123;</span><br><span class="line">                kafkaOperations.send(TOPIC_NAME,<span class="string">"这个是事务里面的消息：1  i="</span>+i);</span><br><span class="line">                <span class="keyword">if</span>(i==<span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"input is error"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                kafkaOperations.send(TOPIC_NAME,<span class="string">"这个是事务里面的消息：2  i="</span>+i);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="关于-Kafka的其他特性和技术选型建议"><a href="#关于-Kafka的其他特性和技术选型建议" class="headerlink" title="关于 Kafka的其他特性和技术选型建议"></a>关于 Kafka的其他特性和技术选型建议</h2><p>Kafka很多内容，但是不一定都要学，看自己的需求，有些功能是比较鸡肋的</p>
<ul>
<li>比如kafka streams 虽然轻量级<ul>
<li>但是与Kafka 紧密联系，无法在没有Kafka 的场景下使用</li>
<li>相较于实时计算工具Spark Streaming、Flink等，kafka streams不适用于大型业务场景</li>
<li>有些功能的话虽然kafka有，但还是用更好的工具比较好，且技术更新换代快，掌握设计思想才主要</li>
<li>更主要的是没有万能的框架，技术选型多数都是基于【 业务需求】出发，选出最合适的技术</li>
<li>kafka/rabbitmq/rocketmq</li>
</ul>
</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Kaluna</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://blog.kaluna.top/2021/06/10/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">https://blog.kaluna.top/2021/06/10/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/kafka/">kafka</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://i.loli.net/2021/04/20/FbPtxEHeSDgCXBy.jpg"><div class="post-qr-code__desc">添加我为好友</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/06/11/%E9%87%8D%E6%96%B0%E5%BB%BA%E7%AB%8B%E5%88%B7%E9%A2%98%E7%9B%AE%E6%A0%87/"><i class="fa fa-chevron-left">  </i><span>重新建立刷题目标</span></a></div><div class="next-post pull-right"><a href="/2021/06/01/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98Redis6.X+%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/"><span>分布式缓存Redis6.X+高可用集群</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'https://blog.kaluna.top/2021/06/10/Kafka%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/';
  this.page.identifier = '2021/06/10/Kafka学习笔记/';
  this.page.title = 'Kafka学习笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'shortname-pzy8cndfkq' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-scr" src="https://shortname-pzy8cndfkq.disqus.com/count.js" async></script></div></div><footer class="footer-bg" style="background-image: url(https://gitee.com/YuerryHUAHUA/figure/raw/master/img/20210521080208.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2021 By Kaluna</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="icp"><a><span></span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>